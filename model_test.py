import torch
import torch.nn as nn
import numpy as np
import pandas as pd
import pickle
import os
import time
import argparse
import logging
from datetime import datetime
from collections import deque
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from model.AnomalyTransformer import AnomalyTransformer
from data_factory.data_loader import CustomCSVSegLoader


class OnlineAnomalyDetector:
    """åœ¨çº¿å¼‚å¸¸æ£€æµ‹å™¨"""
    
    def __init__(self, config):
        self.config = config
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # è®¾ç½®æ—¥å¿—
        self.setup_logging()
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.model = None
        self.scaler = None
        self.threshold = None
        
        # æ»‘åŠ¨çª—å£
        self.window_size = config.win_size
        self.history_window = deque(maxlen=self.window_size)
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.detection_count = 0
        self.anomaly_count = 0
        self.start_time = time.time()
        
        # ç»“æœå­˜å‚¨
        self.results_buffer = deque(maxlen=1000)  # å­˜å‚¨æœ€è¿‘1000ä¸ªæ£€æµ‹ç»“æœ
        
        # åœ¨çº¿æ£€æµ‹çŠ¶æ€
        self.data_index = 0  # å½“å‰å¤„ç†çš„æ•°æ®ç´¢å¼•
        self.data_cache = None  # ç¼“å­˜çš„æ•°æ®
        
        self.logger.info(f"åˆå§‹åŒ–åœ¨çº¿å¼‚å¸¸æ£€æµ‹å™¨å®Œæˆ - è®¾å¤‡: {self.device}")
    
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
        log_dir = "logs"
        os.makedirs(log_dir, exist_ok=True)
        
        log_filename = os.path.join(log_dir, f"anomaly_detection_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log")
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_filename),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def load_model_and_scaler(self):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹å’Œé¢„å¤„ç†å™¨"""
        try:
            # 1. åŠ è½½æ¨¡å‹
            self.model = AnomalyTransformer(
                win_size=self.config.win_size,
                enc_in=self.config.input_c,
                c_out=self.config.output_c,
                e_layers=3
            )
            
            # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(self.config.model_path):
                raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {self.config.model_path}")
            
            # åŠ è½½æ¨¡å‹æƒé‡
            checkpoint = torch.load(self.config.model_path, map_location=self.device)
            self.model.load_state_dict(checkpoint)
            self.model.to(self.device)
            self.model.eval()
            
            self.logger.info(f"æˆåŠŸåŠ è½½æ¨¡å‹: {self.config.model_path}")
            
            # 2. åŠ è½½æˆ–åˆ›å»ºé¢„å¤„ç†å™¨
            scaler_path = self.config.model_path.replace('.pth', '_scaler.pkl')
            if os.path.exists(scaler_path):
                with open(scaler_path, 'rb') as f:
                    self.scaler = pickle.load(f)
                self.logger.info(f"æˆåŠŸåŠ è½½é¢„å¤„ç†å™¨: {scaler_path}")
            else:
                # ä½¿ç”¨è®­ç»ƒæ•°æ®åˆ›å»ºé¢„å¤„ç†å™¨
                self.logger.info("æœªæ‰¾åˆ°é¢„å¤„ç†å™¨æ–‡ä»¶ï¼ŒåŸºäºè®­ç»ƒæ•°æ®åˆ›å»º...")
                self._create_scaler_from_training_data()
            
            # 3. è®¡ç®—é˜ˆå€¼
            self._calculate_threshold()
            
            return True
            
        except Exception as e:
            self.logger.error(f"åŠ è½½æ¨¡å‹å’Œé¢„å¤„ç†å™¨å¤±è´¥: {str(e)}")
            return False
    
    def _create_scaler_from_training_data(self):
        """åŸºäºè®­ç»ƒæ•°æ®åˆ›å»ºé¢„å¤„ç†å™¨"""
        try:
            # ä½¿ç”¨CustomCSVSegLoaderæ¥è·å–è®­ç»ƒæ•°æ®çš„scaler
            train_loader = CustomCSVSegLoader(
                self.config.training_data_path, 
                self.config.win_size, 
                step=1, 
                mode='train'
            )
            self.scaler = train_loader.scaler
            
            # ä¿å­˜scaler
            scaler_path = self.config.model_path.replace('.pth', '_scaler.pkl')
            with open(scaler_path, 'wb') as f:
                pickle.dump(self.scaler, f)
            
            self.logger.info(f"æˆåŠŸåˆ›å»ºå¹¶ä¿å­˜é¢„å¤„ç†å™¨: {scaler_path}")
            
        except Exception as e:
            self.logger.error(f"åˆ›å»ºé¢„å¤„ç†å™¨å¤±è´¥: {str(e)}")
            raise
    
    def _calculate_threshold(self):
        """è®¡ç®—å¼‚å¸¸æ£€æµ‹é˜ˆå€¼"""
        try:
            self.logger.info("å¼€å§‹è®¡ç®—å¼‚å¸¸æ£€æµ‹é˜ˆå€¼...")
            
            # ä½¿ç”¨è®­ç»ƒæ•°æ®è®¡ç®—é˜ˆå€¼
            train_loader = CustomCSVSegLoader(
                self.config.training_data_path,
                self.config.win_size,
                step=self.config.win_size,
                mode='train'
            )
            
            anomaly_scores = []
            temperature = 50
            criterion = nn.MSELoss(reduce=False)
            
            with torch.no_grad():
                for i in range(min(len(train_loader), 100)):  # ä½¿ç”¨å‰100ä¸ªæ ·æœ¬è®¡ç®—é˜ˆå€¼
                    try:
                        data, _ = train_loader[i]
                        input_tensor = torch.FloatTensor(data).unsqueeze(0).to(self.device)
                        
                        output, series, prior, _ = self.model(input_tensor)
                        loss = torch.mean(criterion(input_tensor, output), dim=-1)
                        
                        # è®¡ç®—å¼‚å¸¸åˆ†æ•°
                        series_loss = 0.0
                        prior_loss = 0.0
                        for u in range(len(prior)):
                            if u == 0:
                                series_loss = self._my_kl_loss(series[u], (
                                    prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1, self.config.win_size)
                                ).detach()) * temperature
                                prior_loss = self._my_kl_loss((
                                    prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1, self.config.win_size)
                                ), series[u].detach()) * temperature
                            else:
                                series_loss += self._my_kl_loss(series[u], (
                                    prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1, self.config.win_size)
                                ).detach()) * temperature
                                prior_loss += self._my_kl_loss((
                                    prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1, self.config.win_size)
                                ), series[u].detach()) * temperature
                        
                        metric = torch.softmax((-series_loss - prior_loss), dim=-1)
                        anomaly_score = (metric * loss).detach().cpu().numpy()
                        anomaly_scores.extend(anomaly_score.flatten())
                        
                    except Exception as e:
                        self.logger.warning(f"è®¡ç®—é˜ˆå€¼æ—¶è·³è¿‡ç¬¬{i}ä¸ªæ ·æœ¬: {str(e)}")
                        continue
            
            if anomaly_scores:
                # ä½¿ç”¨æŒ‡å®šçš„å¼‚å¸¸æ¯”ä¾‹è®¡ç®—é˜ˆå€¼
                self.threshold = np.percentile(anomaly_scores, 100 - self.config.anormly_ratio)
                self.logger.info(f"è®¡ç®—å¾—åˆ°å¼‚å¸¸æ£€æµ‹é˜ˆå€¼: {self.threshold:.6f}")
            else:
                # å¦‚æœæ— æ³•è®¡ç®—é˜ˆå€¼ï¼Œä½¿ç”¨é»˜è®¤å€¼
                self.threshold = 0.1
                self.logger.warning(f"æ— æ³•è®¡ç®—é˜ˆå€¼ï¼Œä½¿ç”¨é»˜è®¤å€¼: {self.threshold}")
                
        except Exception as e:
            self.logger.error(f"è®¡ç®—é˜ˆå€¼å¤±è´¥: {str(e)}")
            self.threshold = 0.1
            self.logger.warning(f"ä½¿ç”¨é»˜è®¤é˜ˆå€¼: {self.threshold}")
    
    def _my_kl_loss(self, p, q):
        """KLæ•£åº¦æŸå¤±å‡½æ•°"""
        res = p * (torch.log(p + 0.0001) - torch.log(q + 0.0001))
        return torch.mean(torch.sum(res, dim=-1), dim=1)
    
    def preprocess_data(self, raw_data):
        """é¢„å¤„ç†æ•°æ®"""
        try:
            # ç¡®ä¿æ•°æ®æ˜¯numpyæ•°ç»„
            if isinstance(raw_data, list):
                raw_data = np.array(raw_data)
            
            # é‡å¡‘ä¸ºäºŒç»´æ•°ç»„
            if raw_data.ndim == 1:
                raw_data = raw_data.reshape(-1, 1)
            
            # æ ‡å‡†åŒ–
            processed_data = self.scaler.transform(raw_data)
            return processed_data.flatten()
            
        except Exception as e:
            self.logger.error(f"æ•°æ®é¢„å¤„ç†å¤±è´¥: {str(e)}")
            raise
    
    def detect_anomaly(self, new_data_point, timestamp=None):
        """æ£€æµ‹å•ä¸ªæ•°æ®ç‚¹çš„å¼‚å¸¸"""
        try:
            # é¢„å¤„ç†æ–°æ•°æ®ç‚¹
            processed_point = self.preprocess_data([new_data_point])
            
            # æ›´æ–°æ»‘åŠ¨çª—å£
            self.history_window.append(processed_point[0])
            
            # åªæœ‰å½“çª—å£å¡«æ»¡æ—¶æ‰è¿›è¡Œæ£€æµ‹
            if len(self.history_window) < self.window_size:
                return {
                    'timestamp': timestamp or datetime.now().isoformat(),
                    'raw_value': new_data_point,
                    'is_anomaly': False,
                    'anomaly_score': 0.0,
                    'status': 'warming_up',
                    'window_fill_rate': len(self.history_window) / self.window_size
                }
            
            # è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥æ ¼å¼
            window_data = np.array(list(self.history_window)).reshape(1, self.window_size, 1)
            input_tensor = torch.FloatTensor(window_data).to(self.device)
            
            # æ¨¡å‹æ¨ç†
            with torch.no_grad():
                output, series, prior, _ = self.model(input_tensor)
                
                # è®¡ç®—å¼‚å¸¸åˆ†æ•°
                anomaly_score = self._calculate_anomaly_score(input_tensor, output, series, prior)
                
                # åˆ¤æ–­æ˜¯å¦å¼‚å¸¸
                is_anomaly = anomaly_score > self.threshold
                
                # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                self.detection_count += 1
                if is_anomaly:
                    self.anomaly_count += 1
                
                # æ„å»ºç»“æœ
                result = {
                    'timestamp': timestamp or datetime.now().isoformat(),
                    'raw_value': new_data_point,
                    'processed_value': processed_point[0],
                    'is_anomaly': bool(is_anomaly),
                    'anomaly_score': float(anomaly_score),
                    'threshold': float(self.threshold),
                    'confidence': float(anomaly_score / self.threshold) if self.threshold > 0 else 0.0,
                    'status': 'normal'
                }
                
                # å­˜å‚¨ç»“æœ
                self.results_buffer.append(result)
                
                # è®°å½•å¼‚å¸¸
                if is_anomaly:
                    self.logger.warning(f"æ£€æµ‹åˆ°å¼‚å¸¸! åˆ†æ•°: {anomaly_score:.6f}, é˜ˆå€¼: {self.threshold:.6f}, åŸå§‹å€¼: {new_data_point}")
                
                return result
                
        except Exception as e:
            self.logger.error(f"å¼‚å¸¸æ£€æµ‹å¤±è´¥: {str(e)}")
            return {
                'timestamp': timestamp or datetime.now().isoformat(),
                'raw_value': new_data_point,
                'is_anomaly': False,
                'anomaly_score': 0.0,
                'status': 'error',
                'error_message': str(e)
            }
    
    def _calculate_anomaly_score(self, input_tensor, output, series, prior):
        """è®¡ç®—å¼‚å¸¸åˆ†æ•°"""
        temperature = 50
        criterion = nn.MSELoss(reduction='none')
        
        loss = torch.mean(criterion(input_tensor, output), dim=-1)
        
        series_loss = 0.0
        prior_loss = 0.0
        for u in range(len(prior)):
            if u == 0:
                series_loss = self._my_kl_loss(series[u], (
                    prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1, self.window_size)
                ).detach()) * temperature
                prior_loss = self._my_kl_loss((
                    prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1, self.window_size)
                ), series[u].detach()) * temperature
            else:
                series_loss += self._my_kl_loss(series[u], (
                    prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1, self.window_size)
                ).detach()) * temperature
                prior_loss += self._my_kl_loss((
                    prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1, self.window_size)
                ), series[u].detach()) * temperature
        
        metric = torch.softmax((-series_loss - prior_loss), dim=-1)
        anomaly_score = (metric * loss).detach().cpu().numpy()
        
        return float(np.mean(anomaly_score))
    
    def load_data_for_online_detection(self, csv_path):
        """ä¸ºåœ¨çº¿æ£€æµ‹åŠ è½½æ•°æ®"""
        try:
            df = pd.read_csv(csv_path)
            
            # éªŒè¯CSVæ ¼å¼
            required_columns = ['timestamp', 'value']
            if not all(col in df.columns for col in required_columns):
                raise ValueError(f"CSVæ–‡ä»¶å¿…é¡»åŒ…å«åˆ—: {required_columns}")
            
            # æŒ‰æ—¶é—´æˆ³æ’åº
            df['timestamp'] = pd.to_datetime(df['timestamp'])
            df = df.sort_values('timestamp').reset_index(drop=True)
            
            self.data_cache = df
            self.data_index = 0
            
            self.logger.info(f"æˆåŠŸåŠ è½½æ•°æ®æ–‡ä»¶: {csv_path}, å…± {len(df)} æ¡è®°å½•")
            return True
            
        except Exception as e:
            self.logger.error(f"åŠ è½½æ•°æ®æ–‡ä»¶å¤±è´¥: {str(e)}")
            return False
    
    def batch_detect_from_csv(self, csv_path):
        """ä»CSVæ–‡ä»¶æ‰¹é‡æ£€æµ‹å¼‚å¸¸"""
        try:
            self.logger.info(f"å¼€å§‹æ‰¹é‡æ£€æµ‹: {csv_path}")
            
            # è¯»å–CSVæ–‡ä»¶
            df = pd.read_csv(csv_path)
            
            # éªŒè¯CSVæ ¼å¼
            required_columns = ['timestamp', 'value']
            if not all(col in df.columns for col in required_columns):
                raise ValueError(f"CSVæ–‡ä»¶å¿…é¡»åŒ…å«åˆ—: {required_columns}")
            
            # æŒ‰æ—¶é—´æˆ³æ’åº
            df['timestamp'] = pd.to_datetime(df['timestamp'])
            df = df.sort_values('timestamp').reset_index(drop=True)
            
            results = []
            total_rows = len(df)
            
            self.logger.info(f"å¼€å§‹å¤„ç† {total_rows} æ¡æ•°æ®...")
            
            for idx, row in df.iterrows():
                result = self.detect_anomaly(row['value'], row['timestamp'].isoformat())
                
                # å¦‚æœæœ‰æ ‡ç­¾åˆ—ï¼Œæ·»åŠ çœŸå®æ ‡ç­¾
                if 'label' in df.columns:
                    result['true_label'] = int(row['label'])
                
                results.append(result)
                
                # è¿›åº¦æŠ¥å‘Š
                if (idx + 1) % 100 == 0:
                    progress = (idx + 1) / total_rows * 100
                    self.logger.info(f"å¤„ç†è¿›åº¦: {progress:.1f}% ({idx + 1}/{total_rows})")
            
            self.logger.info(f"æ‰¹é‡æ£€æµ‹å®Œæˆï¼Œæ€»è®¡å¤„ç† {len(results)} æ¡æ•°æ®")
            return results
            
        except Exception as e:
            self.logger.error(f"æ‰¹é‡æ£€æµ‹å¤±è´¥: {str(e)}")
            raise
    
    def evaluate_performance(self, results):
        """è¯„ä¼°æ£€æµ‹æ€§èƒ½"""
        try:
            # ç­›é€‰æœ‰çœŸå®æ ‡ç­¾çš„ç»“æœ
            labeled_results = [r for r in results if 'true_label' in r and r['status'] != 'error']
            
            if not labeled_results:
                self.logger.warning("æ²¡æœ‰æ ‡ç­¾æ•°æ®ï¼Œæ— æ³•è®¡ç®—æ€§èƒ½æŒ‡æ ‡")
                return None
            
            # æå–é¢„æµ‹å’ŒçœŸå®æ ‡ç­¾
            y_true = [r['true_label'] for r in labeled_results]
            y_pred = [int(r['is_anomaly']) for r in labeled_results]
            
            # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
            accuracy = accuracy_score(y_true, y_pred)
            precision, recall, f1_score, _ = precision_recall_fscore_support(
                y_true, y_pred, average='binary', zero_division=0
            )
            
            performance = {
                'total_samples': len(labeled_results),
                'true_positives': sum(1 for i in range(len(y_true)) if y_true[i] == 1 and y_pred[i] == 1),
                'false_positives': sum(1 for i in range(len(y_true)) if y_true[i] == 0 and y_pred[i] == 1),
                'true_negatives': sum(1 for i in range(len(y_true)) if y_true[i] == 0 and y_pred[i] == 0),
                'false_negatives': sum(1 for i in range(len(y_true)) if y_true[i] == 1 and y_pred[i] == 0),
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'f1_score': f1_score
            }
            
            self.logger.info("=" * 50)
            self.logger.info("æ€§èƒ½è¯„ä¼°ç»“æœ:")
            self.logger.info(f"å‡†ç¡®ç‡ (Accuracy):  {accuracy:.4f}")
            self.logger.info(f"ç²¾ç¡®ç‡ (Precision): {precision:.4f}")
            self.logger.info(f"å¬å›ç‡ (Recall):    {recall:.4f}")
            self.logger.info(f"F1åˆ†æ•° (F1-Score):  {f1_score:.4f}")
            self.logger.info("=" * 50)
            
            return performance
            
        except Exception as e:
            self.logger.error(f"æ€§èƒ½è¯„ä¼°å¤±è´¥: {str(e)}")
            return None
    
    def save_results(self, results, output_path):
        """ä¿å­˜æ£€æµ‹ç»“æœ"""
        try:
            df = pd.DataFrame(results)
            df.to_csv(output_path, index=False)
            self.logger.info(f"ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜ç»“æœå¤±è´¥: {str(e)}")
    
    def get_statistics(self):
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        runtime = time.time() - self.start_time
        anomaly_rate = (self.anomaly_count / self.detection_count * 100) if self.detection_count > 0 else 0
        detection_rate = self.detection_count / runtime if runtime > 0 else 0
        
        return {
            'total_detections': self.detection_count,
            'total_anomalies': self.anomaly_count,
            'anomaly_rate': anomaly_rate,
            'runtime_seconds': runtime,
            'detection_rate_per_second': detection_rate,
            'threshold': self.threshold,
            'device': str(self.device)
        }
    
    def run_online_detection(self, data_source_path, detection_interval=1):
        """è¿è¡Œåœ¨çº¿å¼‚å¸¸æ£€æµ‹ - ä¿®æ­£ç‰ˆæœ¬"""
        self.logger.info("å¼€å§‹åœ¨çº¿å¼‚å¸¸æ£€æµ‹æ¨¡å¼...")
        
        # åŠ è½½æ•°æ®æ–‡ä»¶
        if not self.load_data_for_online_detection(data_source_path):
            self.logger.error("æ— æ³•åŠ è½½æ•°æ®æ–‡ä»¶ï¼Œé€€å‡ºåœ¨çº¿æ£€æµ‹")
            return
        
        try:
            while self.data_index < len(self.data_cache):
                # è·å–å½“å‰æ•°æ®ç‚¹
                current_row = self.data_cache.iloc[self.data_index]
                timestamp = current_row['timestamp'].isoformat()
                value = current_row['value']
                
                # æ£€æµ‹å¼‚å¸¸
                result = self.detect_anomaly(value, timestamp)
                
                # è¾“å‡ºç»“æœ
                status_icon = "âš ï¸" if result['is_anomaly'] else "âœ…"
                status_text = "å¼‚å¸¸æ£€æµ‹" if result['is_anomaly'] else "æ­£å¸¸æ•°æ®"
                
                print(f"{status_icon} {status_text} - æ—¶é—´: {timestamp}, å€¼: {value:.6f}, åˆ†æ•°: {result['anomaly_score']:.6f}")
                
                # å¦‚æœæ˜¯å¼‚å¸¸ï¼Œé¢å¤–è®°å½•è¯¦ç»†ä¿¡æ¯
                if result['is_anomaly']:
                    print(f"    ç½®ä¿¡åº¦: {result['confidence']:.2f}, é˜ˆå€¼: {result['threshold']:.6f}")
                
                # ç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ªæ•°æ®ç‚¹
                self.data_index += 1
                
                # ç­‰å¾…ä¸‹æ¬¡æ£€æµ‹
                time.sleep(detection_interval)
                
        except KeyboardInterrupt:
            self.logger.info("åœ¨çº¿æ£€æµ‹å·²åœæ­¢")
        finally:
            # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
            stats = self.get_statistics()
            self.logger.info(f"æ£€æµ‹ç»Ÿè®¡: {stats}")
            print(f"\n æ£€æµ‹å®Œæˆç»Ÿè®¡:")
            print(f"   æ€»æ£€æµ‹æ•°: {stats['total_detections']}")
            print(f"   å¼‚å¸¸æ•°é‡: {stats['total_anomalies']}")
            print(f"   å¼‚å¸¸ç‡: {stats['anomaly_rate']:.2f}%")
            print(f"   å¤„ç†æ•°æ®ç´¢å¼•: {self.data_index}/{len(self.data_cache) if self.data_cache is not None else 0}")


def main():
    parser = argparse.ArgumentParser(description='åœ¨çº¿å¼‚å¸¸æ£€æµ‹ç³»ç»Ÿ')
    
    # æ¨¡å‹å’Œæ•°æ®å‚æ•°
    parser.add_argument('--model_path', type=str, 
                        default='checkpoints/credit_checkpoint.pth',
                        help='è®­ç»ƒå¥½çš„æ¨¡å‹æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--training_data_path', type=str,
                        default='data/YT.11PI_04019.PV.csv',
                        help='è®­ç»ƒæ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆç”¨äºåˆ›å»ºé¢„å¤„ç†å™¨å’Œè®¡ç®—é˜ˆå€¼ï¼‰')
    parser.add_argument('--test_data_path', type=str,
                        default='data/YT.11PI_45201.PV.csv',
                        help='æµ‹è¯•æ•°æ®æ–‡ä»¶è·¯å¾„')
    
    # æ¨¡å‹é…ç½®å‚æ•°
    parser.add_argument('--win_size', type=int, default=100,
                        help='æ—¶é—´çª—å£å¤§å°')
    parser.add_argument('--input_c', type=int, default=1,
                        help='è¾“å…¥é€šé“æ•°')
    parser.add_argument('--output_c', type=int, default=1,
                        help='è¾“å‡ºé€šé“æ•°')
    parser.add_argument('--anormly_ratio', type=float, default=0.5,
                        help='å¼‚å¸¸æ¯”ä¾‹ï¼ˆç”¨äºè®¡ç®—é˜ˆå€¼ï¼‰')
    
    # è¿è¡Œæ¨¡å¼
    parser.add_argument('--mode', type=str, 
                        choices=['batch', 'online', 'test'],
                        default='online',
                        help='è¿è¡Œæ¨¡å¼: batch-æ‰¹é‡æ£€æµ‹, online-åœ¨çº¿æ£€æµ‹, test-æµ‹è¯•æ¨¡å¼')
    parser.add_argument('--output_path', type=str,
                        default='detection_results.csv',
                        help='ç»“æœè¾“å‡ºè·¯å¾„')
    parser.add_argument('--detection_interval', type=int, default=1,
                        help='åœ¨çº¿æ£€æµ‹é—´éš”ï¼ˆç§’ï¼‰')
    
    args = parser.parse_args()
    
    # åˆ›å»ºæ£€æµ‹å™¨
    detector = OnlineAnomalyDetector(args)
    
    # åŠ è½½æ¨¡å‹å’Œé¢„å¤„ç†å™¨
    if not detector.load_model_and_scaler():
        print("âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œç¨‹åºé€€å‡º")
        return
    
    print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
    
    try:
        if args.mode == 'batch':
            # æ‰¹é‡æ£€æµ‹æ¨¡å¼
            print(f"ğŸ” å¼€å§‹æ‰¹é‡æ£€æµ‹: {args.test_data_path}")
            results = detector.batch_detect_from_csv(args.test_data_path)
            
            # ä¿å­˜ç»“æœ
            detector.save_results(results, args.output_path)
            
            # è¯„ä¼°æ€§èƒ½
            performance = detector.evaluate_performance(results)
            
            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            stats = detector.get_statistics()
            print("\n æ£€æµ‹ç»Ÿè®¡:")
            for key, value in stats.items():
                print(f"  {key}: {value}")
            
        elif args.mode == 'online':
            # åœ¨çº¿æ£€æµ‹æ¨¡å¼
            print(f"å¯åŠ¨åœ¨çº¿æ£€æµ‹æ¨¡å¼ï¼Œæ•°æ®æ–‡ä»¶: {args.test_data_path}")
            print(f"   æ£€æµ‹é—´éš”: {args.detection_interval}ç§’")
            print("   æŒ‰ Ctrl+C åœæ­¢æ£€æµ‹")
            detector.run_online_detection(args.test_data_path, args.detection_interval)
            
        elif args.mode == 'test':
            # æµ‹è¯•æ¨¡å¼ - æ£€æµ‹å‡ ä¸ªæ ·æœ¬
            print("æµ‹è¯•æ¨¡å¼ - æ£€æµ‹å‰10ä¸ªæ ·æœ¬")
            df = pd.read_csv(args.test_data_path)
            
            for i in range(min(10, len(df))):
                timestamp = df['timestamp'].iloc[i] if 'timestamp' in df.columns else None
                value = df['value'].iloc[i]
                result = detector.detect_anomaly(value, timestamp)
                
                status_icon = "âš ï¸" if result['is_anomaly'] else "âœ…"
                print(f"{status_icon} æ ·æœ¬ {i+1}: æ—¶é—´={timestamp}, å€¼={value:.6f}, åˆ†æ•°={result['anomaly_score']:.6f}, å¼‚å¸¸={result['is_anomaly']}")
    
    except Exception as e:
        print(f"âŒ æ‰§è¡Œå¤±è´¥: {str(e)}")
        detector.logger.error(f"æ‰§è¡Œå¤±è´¥: {str(e)}")
    
    finally:
        # è¾“å‡ºæœ€ç»ˆç»Ÿè®¡ä¿¡æ¯
        stats = detector.get_statistics()
        print(f"\n æœ€ç»ˆç»Ÿè®¡: æ€»æ£€æµ‹={stats['total_detections']}, å¼‚å¸¸={stats['total_anomalies']}, å¼‚å¸¸ç‡={stats['anomaly_rate']:.2f}%")


if __name__ == "__main__":
    main()
